#+begin_export latex
\section{Introduction}
The program \ty{prepAlfy} takes as input a directory of query fasta
files and a directory of subject fasta files. \ty{prepAlfy} will then
calculate the match lengths between the queries and subjects. The
output file is a two column table consisting of the query name,
followed by pairs of match lengths and sequence identifiers.
#+end_export
#+begin_export latex
\section{Implementation}

The outline for \ty{preAlfy} contains hooks for imports,
structures, functions and the logic of the main function.
#+end_export
#+begin_src go <<prepAlfy.go>>=
  package main

  import (
	  //<<Imports, Ch. \ref{ch:prep}>>
  )
  //<<Types, Ch. \ref{ch:prep}>>
  //<<Functions, Ch. \ref{ch:prep}>>
  func main() {	 
	  //<<Main function, Ch. \ref{ch:prep}>>	
  }
#+end_src
#+begin_export latex
In the main function we prepare the error handling, declare the
options, set the usage, parse the options and construct the index
tables.
#+end_export
#+begin_src go <<Main function, Ch. \ref{ch:prep}>>=
  //<<Prepare error handling, Ch. \ref{ch:prep}>>
  //<<Declare options, Ch. \ref{ch:prep}>>
  //<<Set usage, Ch. \ref{ch:prep}>>
  //<<Parse options, Ch. \ref{ch:prep}>>
  //<<Construct alfy input table, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
We take care of error handling by calling the function
\ty{PrepareErrorMessages} from the \ty{util} package.
#+end_export
#+begin_src go <<Prepare error handling, Ch. \ref{ch:prep}>>=
util.PrepareErrorMessages("prepAlfy")
#+end_src
#+begin_export latex
We declare options for printing the version, picking the query and
subject directories and the number of threads. By default this
corresponds to the number of logical CPUs available on the system.
#+end_export
#+begin_src go <<Declare options, Ch. \ref{ch:prep}>>=
  optV := flag.Bool("v", false, "version")
  optQ := flag.String("q", "", "query directory")
  optS := flag.String("s", "", "subject directory")
  ncpu := runtime.NumCPU()
  optT := flag.Int("t", ncpu , "number of threads")
#+end_src
#+begin_export latex
We import flag and runtime.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "flag"
  "runtime"
#+end_src
#+begin_export latex
The usage consists of three parts, the actual usage message, an
explanation of the purpose of \ty{prepAlfy}, and an example command.
#+end_export
#+begin_src go <<Set usage, Ch. \ref{ch:prep}>>=
  u :="prepAlfy -q <queryDir> -s <subjectDir>"
  p :="Prepare alfy input"
  e :="prepAlfy -q queries/ -s subjects/"
  clio.Usage(u,p,e)
#+end_src
#+begin_export latex
We import clio.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/clio"
#+end_src
#+begin_export latex
We parse the options and respond to the version option, -v, the two
directory options, \ty{-q} and \ty{-s}, and the number of threads,
\ty{-t}.
#+end_export
#+begin_src go <<Parse options, Ch. \ref{ch:prep}>>=
  flag.Parse()
  //<<Respond to \ty{-v}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-q}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-s}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-t}, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
If the user requested the version, we call the utility function
\ty{Version} with the program name.
#+end_export
#+begin_src go <<Respond to \ty{-v}, Ch. \ref{ch:prep}>>=
  if *optV {
	  util.Version("prepAlfy")
  }
#+end_src
#+begin_export latex
We import util.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/alfy/util"
#+end_src
#+begin_export latex
If the user didn't set a query directory, we exit with a friendly
message.
#+end_export
#+begin_src go <<Respond to \ty{-q}, Ch. \ref{ch:prep}>>=
  if *optQ == "" {
	  m := "please provide a directory " +
		  "of query sequences"
	  fmt.Fprintf(os.Stderr,"%s\n",m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
We import fmt and os.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "fmt"
  "os"
#+end_src
#+begin_export latex
We also exit with a friendly message if the subject directory was not
set.
#+end_export
#+begin_src go <<Respond to \ty{-s}, Ch. \ref{ch:prep}>>=
  if *optS == "" {
	  m := "please provide a directory " +
		  "of subject sequences"
	  fmt.Fprintf(os.Stderr,"%s\n",m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
If the user set fewer than one thread, we throw a fatal error.
#+end_export
#+begin_src go <<Respond to \ty{-t}, Ch. \ref{ch:prep}>>=
  if *optT <= 0 {
	  log.Fatalf("Can't set %d threads.", *optT)
  }
#+end_src
#+begin_export latex
We import log.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "log"
#+end_src
#+begin_export latex
To construct the alfy input table, we read the names of the queries
and subjects and make sure there are no overlaps. This is done by
storing the names of the files in maps. Since maps aren't ordered, we
also sort the names beforehand.

Then we construct the elements required for the output table: the
query name, the subject ID-key pairing and the matches lengths.
#+end_export
#+begin_src go <<Construct alfy input table, Ch. \ref{ch:prep}>>=
  //<<Read names of queries and subjects, Ch. \ref{ch:prep}>>
  //<<Sort names of queries and subjects, Ch. \ref{ch:prep}>>
  //<<Do queries and subjects overlap, Ch. \ref{ch:prep}>>
  //<<Write match factors, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
The reading of the queries and subjects is done by the function
readDir. If either of the directories are empty, we exit and warn the
user.
#+end_export
#+begin_src go <<Read names of queries and subjects, Ch. \ref{ch:prep}>>=
  //<<Read names of queries, Ch. \ref{ch:prep}>>
  //<<Read names of subjects, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
We start by reading the query directory.
#+end_export
#+begin_src go <<Read names of queries, Ch. \ref{ch:prep}>>=
  queries := readDir(*optQ)
  if len(queries) == 0 {
	  fmt.Fprintf(os.Stderr, "%s is empty\n", *optQ)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
We then read the subject directory.
#+end_export
#+begin_src go <<Read names of subjects, Ch. \ref{ch:prep}>>=
  subjects := readDir(*optS)
  if len(subjects) == 0 {
	  fmt.Fprintf(os.Stderr, "%s is empty\n", *optS)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
The function \ty{readDir} takes as input the name of a directory and
returns a map of the names of the sequence files within the
directory. We check each directory entry before storing its name.
#+end_export
#+begin_src go <<Functions, Ch. \ref{ch:prep}>>=
  func readDir(dir string) map[string]bool {
	  dirEntries, err := os.ReadDir(dir)
	  util.Check(err)
	  names := make(map[string]bool)
	  for _, dirEntry := range dirEntries {
		  //<<Check directory entry, Ch. \ref{ch:prep}>>
		  names[dirEntry.Name()] = true
	  }
	  return names
  }
#+end_src
#+begin_export latex
Directory entries should consists of files and not subdirectories. The
files themselves should be nucleotide FASTA files.
#+end_export
#+begin_src go <<Check directory entry, Ch. \ref{ch:prep}>>=
  //<<Check if entry is a file, Ch. \ref{ch:prep}>>
  //<<Check if entry is nucleotide FASTA, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
If the entry is a subdirectory, we skip it and warn the user.
#+end_export
#+begin_src go <<Check if entry is a file, Ch. \ref{ch:prep}>>=
  if dirEntry.IsDir() {
	  p := dir + "/" + dirEntry.Name()
	  fmt.Fprintf(os.Stderr,
		  "Skipping subdirectory %s\n", p)
	  continue
  }
#+end_src
#+begin_export latex
Then we check if the entry is a nucleotide FASTA format. There are
currently five filenames extensions that can refer to nucleotide FASTA
files listed on
Wikipedia\footnote{\ty{https://en.wikipedia.org/wiki/FASTA\_format}}:
fasta, fna, ffn, frn and fa. If the file has a different extension, or
none, we skip it and warn the user.
#+end_export
#+begin_src go <<Check if entry is nucleotide FASTA, Ch. \ref{ch:prep}>>=
  ext := filepath.Ext(dirEntry.Name())
  if ext != ".fasta" && ext != ".fna" &&
	  ext != ".ffn" && ext != ".fnr" &&
	  ext != ".fa" {
	  m := "%s doesnt have the extension of a " +
		  "nucleotide FASTA file." +
		  "Skipping it \n"
	  p := dir + "/" + dirEntry.Name()
	  fmt.Fprintf(os.Stderr, m, p)
	  continue
  }
#+end_src
#+begin_export latex
We import filepath.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "path/filepath"
#+end_src
#+begin_export latex
We store the names of the queries in slices and sort them. Then we
store the subject IDs.
#+end_export
#+begin_src go <<Sort names of queries and subjects, Ch. \ref{ch:prep}>>=
  var queryNames, subjectNames []string
  var subjectIDs []int
  //<<Sort names of queries, Ch. \ref{ch:prep}>>
  //<<Sort names of subjects, Ch. \ref{ch:prep}>>
  //<<Store subject IDs, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
First we sort the queries.
#+end_export
#+begin_src go <<Sort names of queries, Ch. \ref{ch:prep}>>=
  for query := range queries {
	  queryNames = append(queryNames, query)
  }
  sort.Strings(queryNames)
#+end_src
#+begin_export latex
We import sort.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "sort"
#+end_src
#+begin_export latex
Then we do the same for the subjects.
#+end_export
#+begin_src go <<Sort names of subjects, Ch. \ref{ch:prep}>>=
  for subject := range subjects {
	  subjectNames = append(subjectNames, subject)
  }
  sort.Strings(subjectNames)
#+end_src
#+begin_export latex
The subject IDs are simply the indexes in the slice of subject names.
#+end_export
#+begin_src go <<Store subject IDs, Ch. \ref{ch:prep}>>=
  for i, _ := range subjectNames {
	  subjectIDs = append(subjectIDs, i)
  }
#+end_src
#+begin_export latex
We then check if there is an overlap in the queries and subject
files. To do so, we search for pairs of repeated file names and exit
with a message if we find one.
#+end_export
#+begin_src go <<Do queries and subjects overlap, Ch. \ref{ch:prep}>>=           
  for _, query := range queryNames {
	  if subjects[query] {
		  m := "Found %s%s and %s%s." +
			  "Please ensure queries and " +
			  "subjects do not " +
			  "overlap."
		  fmt.Fprintf(os.Stderr, m, *optQ, query,
			  *optS, query)
		  os.Exit(1)
	  }
  }
#+end_src
#+begin_export latex
\subsection{Write match factors}

We now move on to the calculation of the match factors. This is main
algorithm of \ty{prepAlfy}. Since subject sequences may be composed of
several short contigs, it is more efficient to concatenate shorter
contigs into one larger sequence. To do so, we first determine the
maximum subject length. This value will be later using during the
streaming step where we stream the queries against the subjects. Since
the maximum length only needs to be calculated once, we do it before
we iterate over the query sequences. Each query is to be streamed
against the subjects to calculate the matches lengths. This process is
distributed among goroutines. The distribution of the sequences across
the goroutines will depend on the maximum subject length previously
calculated.

The subjects will be streamed against each query at a time. After
streaming the subjects the resulting match lengths are merged while
taking note of the subject they were found in. We then calculate the
enhanced suffix array, or esa, for each one of the subject files. At
the end the pair match length and subject ID is written out to a file.
#+end_export
#+begin_src go <<Write match factors, Ch. \ref{ch:prep}>>=
  //<<Find maximum subject length, Ch. \ref{ch:prep}>>
  //<<Calculate enhanced suffix arrays, Ch. \ref{ch:prep}>>
  //<<Read query sequences, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
To find the maximal subject length, we iterate over the subject files
and for each file iterate over the sequences it contains to find the
longest subject file. 
#+end_export
#+begin_src go <<Find maximum subject length, Ch. \ref{ch:prep}>>=
  msl := -1
  for _, subject := range subjectNames {
	  p := *optS + "/" + subject
	  f, err := os.Open(p)
	  util.Check(err)
	  //<<Iterate over subjects, Ch. \ref{ch:prep}>>
	  f.Close()
  }
#+end_src
#+begin_export latex
We use a scanner to iterate over the sequences in the current neighbor file.
#+end_export
#+begin_src go <<Iterate over subjects, Ch. \ref{ch:prep}>>=
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  l := len(sc.Sequence().Data())
	  if l > msl {
		  msl = l
	  }
  }
#+end_src
#+begin_export latex
We import fasta
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"github.com/evolbioinf/fasta"
#+end_src
#+begin_export latex
We then calculate the enhanced suffix arrays for all subject
files. First we create a map between the attributed numerical subject
ID and the subject filename. We also create a variable that will hold
the calculated esas. Then we calculate the esa. This is done only once
for all subject files. For this we read the subject files, and map the
subject file name to a numerical ID.
#+end_export
#+begin_src go <<Calculate enhanced suffix arrays, Ch. \ref{ch:prep}>>= 
  sID := make(map[int]string)
  esas := make([]*esa.Esa,len(subjectNames))
  for i, subject := range subjectNames {
	  sID[i] = subject
	  p := *optS + "/" + subject
	  f, err := os.Open(p)
	  util.Check(err)
	  //<<Read subject file, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
While reading the subject file we check the length of the contig. If
it is shorter than the maximum subject length calculated above we
concatenate it. The esa is calculated on the concatenated sequences.
#+end_export
#+begin_src go <<Read subject file, Ch. \ref{ch:prep}>>=
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  s := sc.Sequence()
	  d := s.Data()
	  h := []byte(s.Header())
	  for len(d) < msl && sc.ScanSequence() {
		  s = sc.Sequence()
		  h = append(h, '|')
		  h = append(h, []byte(s.Header())...)
		  d = append(d, s.Data()...)
	  }
	  //<<Calculate enhanced suffix array, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We calculate the enhanced suffix array or \ty{esa} of the subject
sequences.
#+end_export
#+begin_src go <<Calculate enhanced suffix array, Ch. \ref{ch:prep}>>=
  d = bytes.ToUpper(d)
  esas[i] = esa.MakeEsa(d)
#+end_src
#+begin_export latex
We import esa.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/esa"
#+end_src
#+begin_export latex
We then start reading the query sequences. For each sequence we
calculate the forward and the reverse strands, and we remove the
masking information from the sequence. Then we iterate over their
sequences and stream the subjects against it. After streaming all the
subjects we update the final matches lengths and write it to a file.
#+end_export
#+begin_src go <<Read query sequences, Ch. \ref{ch:prep}>>=
  for query, _ := range queries {
	  //<<Store forward and reverse strand, Ch. \ref{ch:prep}>>
	  //<<Remove masking and reverse sequence, Ch. \ref{ch:prep}>>
	  //<<Distribute streaming across goroutines, Ch. \ref{ch:prep}>>
	  //<<Merge matches results, Ch. \ref{ch:prep}>>
	  //<<Write matches to stdout, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We read the query files and we store the forward and the reverse
complement of the sequence. We close the file at the end.
#+end_export
#+begin_src go <<Store forward and reverse strand, Ch. \ref{ch:prep}>>= 
  p := *optQ + "/" + query
  f, err := os.Open(p)
  util.Check(err)
  var querySeqs, revQuerySeqs []*fasta.Sequence
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  s := sc.Sequence()
	  querySeqs = append(querySeqs, s)
	  s = fasta.NewSequence(s.Header(), s.Data())
	  s.ReverseComplement()
	  revQuerySeqs = append(revQuerySeqs, s)
  }
  f.Close()
#+end_src
#+begin_export latex
Since we are interested in exact matching we convert all nucleotides
to uppercase, removing the masking information from the sequence. Both
in the forward and in the reverse strand.
#+end_export
#+begin_src go <<Remove masking and reverse sequence, Ch. \ref{ch:prep}>>=
  for i, querySeq := range querySeqs {
	  h := querySeq.Header()
	  d := bytes.ToUpper(querySeq.Data())
	  querySeqs[i] = fasta.NewSequence(h, d)
	  h = revQuerySeqs[i].Header()
	  d = bytes.ToUpper(revQuerySeqs[i].Data())
	  revQuerySeqs[i] = fasta.NewSequence(h, d)
  }
#+end_src
#+begin_export latex
We import byte.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"bytes"
#+end_src
#+begin_export latex
Since streaming is the rate limiting step, we distribute the streaming
of the subjects across goroutines. For this we start by constructing a
structure to hold both the match lengths and the subjects ID. This
ensures both values are passed on together.
#+end_export
#+begin_src go <<Types, Ch. \ref{ch:prep}>>=
  type Matches struct {
	  matchLengths [][]int
	  subjectID [][]int
  }
#+end_src
#+begin_export latex
We distribute the streaming work along sets of neighbors files. Thus,
we split the list of neighbor files into packages of size n. We then
concurrently stream the query against the sets of subject files. The
concurrency pattern is based on \ty{sync.WaitGroup}, and is built from
a channel and a \ty{WaitGroup}, both of which we initialize outside of
the loop from which the goroutines are started. Inside the loop we
increment the counter of working goroutines and construct the
goroutine for streaming. After the loop we construct the closer and
the iterator that belong to the \ty{WaitGroup} concurrency pattern.
#+end_export
#+begin_src go <<Distribute streaming across goroutines, Ch. \ref{ch:prep}>>=
  EsaSets := make([][]*esa.Esa, 0)
  subjectIDsets := make([][]int, 0)
  //<<Split subject filenames, Ch. \ref{ch:prep}>>
  matchesSets := make(chan Matches)
  var wg sync.WaitGroup
  for i, Esa := range EsaSets{
	  subjectIDs := subjectIDsets[i]
	  wg.Add(1)
	  //<<Goroutine for streaming, Ch. \ref{ch:prep}>>
  }
  //<<Streaming closer, Ch. \ref{ch:prep}>>
  //<<Streaming iterator, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
We import sync.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"sync"
#+end_src
#+begin_export latex
The subject files are split into several sets such that $\ell=n/T$,
where $n$ is the number of sets and $T$ is the number of threads. We
also ensure that the last of the slices does not extend beyond the
length of the slice of subject names.
#+end_export
#+begin_src go <<Split subject filenames, Ch. \ref{ch:prep}>>=
  n := len(subjectNames)
  length := int(math.Ceil(float64(n)/float64(*optT)))
  start := 0
  end := length
  for start < n {
	  EsaSets = append(EsaSets,
		  esas[start:end])
	  subjectIDsets = append(subjectIDsets,
		  subjectIDs[start:end])
	  start = end
	  end += length
	  if end > n {
		  end = n
	  }
  }
#+end_src
#+begin_export latex
We import math.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"math"
#+end_src
#+begin_export latex
We pass the current set of subject names and subject IDs to the
goroutine. Inside it we defer it closure. We also declare and allocate
space for holding the matches lengths and subject IDs, stream the
query against the subjects and pass the matches along the channel.
The array of matches lengths and subjects IDs is the size of the
number of query sequences.
#+end_export
#+begin_src go <<Goroutine for streaming, Ch. \ref{ch:prep}>>=
  go func(Esa []*esa.Esa, subjectIDs []int){
	  defer wg.Done()
	  var matches Matches
	  n :=len(querySeqs)
	  matches.matchLengths = make([][]int,n)
	  matches.subjectID = make([][]int,n)
	  //<<Allocate space for match lengths and subject ID, Ch. \ref{ch:prep}>>
	  matchesSets <- matches
  }(Esa, subjectIDs)
#+end_src
#+begin_export latex
Then we iterate over the range of existing sequences and allocate
space for the array of matches lengths and for the array of subject
IDs for each query, which has a dimension m, the size of the length of
the query sequences.
#+end_export
#+begin_src go <<Allocate space for match lengths and subject ID, Ch. \ref{ch:prep}>>=
  for i, querySeq := range querySeqs {
	  m := len(querySeq.Data())
	  matches.matchLengths[i] = make([]int,m)
	  matches.subjectID[i] = make([]int,m)
	  //<<Stream against subject files, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We then iterate, map the subject files to an ID and read the
sequences. After streaming the files we write the matches lengths to a
file.
#+end_export
#+begin_src go <<Stream against subject files, Ch. \ref{ch:prep}>>=
  for j, esa := range Esa {
	  //<<Update match lengths and IDs, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We create a map of the subject files.
#+end_export
#+begin_export latex
We first read the subject files, then we aggregate smaller sequences
or contigs until they reach the maximum subject length, \ty{msl},
which was calculated above. Note that the aggregation is done only
within files and not across.

After aggregating the sequences we calculate the enhanced suffix array
of the subject and calculate the matches lengths.
#+end_export
#+begin_export latex
We update the array of matches lengths and subjects IDs.
#+end_export
#+begin_src go <<Update match lengths and IDs, Ch. \ref{ch:prep}>>=
  q := querySeq.Data()
//  println(esa)
  mat.UpdateMatchLengths(q, esa, subjectIDs[j],
	  matches.matchLengths[i],
	  matches.subjectID[i])
#+end_src
#+begin_export latex
We import the mat package.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/alfy/mat"
#+end_src
#+begin_export latex
We reach the end of the goroutine. In the closer we wait until all
working goroutines have finished and then close the channel.
#+end_export
#+begin_src go <<Streaming closer, Ch. \ref{ch:prep}>>=
  go func() {
	  wg.Wait()
	  close(matchesSets)
  }()
#+end_src
#+begin_export latex
In the iterator we pack the matches structure with the results from
the channel. The results from the matches lengths and the subject IDs
are stored in a master set of lengths.
#+end_export
#+begin_src go <<Streaming iterator, Ch. \ref{ch:prep}>>=
  matchLengths := make([][]int,0)
    subjectIDs := make([][]int,0)
    for _, qs := range querySeqs {
	    n := len(qs.Data())
	    ml := make([]int, n)
	    matchLengths = append(matchLengths, ml)
	    n = len(qs.Data())
	    ml = make([]int, n)
	    subjectIDs = append(subjectIDs,ml)
    }
    for match := range matchesSets {
	    for i := 0; i < len(match.matchLengths); i++ {
		  for j := 0;
		    j < len(match.matchLengths[i]); j++ {
		  }
	    }
	    <<Store match lengths, Ch. \ref{ch:prep}>>
    }
#+end_src
#+begin_export latex
We store the longer of the matches for each position and the
corresponding subject ID.
#+end_export
#+begin_src go <<Store match lengths, Ch. \ref{ch:prep}>>=
  for i, lengths := range match.matchLengths {
	  for j, length := range lengths {
		  if matchLengths[i][j] < length {
			  matchLengths[i][j] = length
			  subjectIDs[i][j] =
				  match.subjectID[i][j]
		  }
	  }
  }
#+end_src
#+begin_export latex
We then merge the matches from the several routines. We go through
every position and note down the length of the longest match.
#+end_export
#+begin_src go <<Merge matches results, Ch. \ref{ch:prep}>>=
  for i, ml := range matchLengths {
	  l := 0
	  id := 0
	  for j := 0; j < len(ml); j++ {
		  if ml[j] > l {
			  l = ml[j]
			  id = subjectIDs[i][j]
		  }
		  ml[j] = l
		  subjectIDs[i][j] = id
		  l--
	  }
  }
#+end_src
#+begin_export latex
After finishing streaming the subject files and updating the arrays of
matches lengths and IDs we write them to standard out.
#+end_export
#+begin_src go <<Write matches to stdout, Ch. \ref{ch:prep}>>=
   for i := range querySeqs {
	   ml := matchLengths[i]
	   id := subjectIDs[i]
	   wr := bufio.NewWriter(os.Stdout)
	   //<<Write output header, Ch. \ref{ch:prep}>>
	   //<<Write matches and subject ID, Ch. \ref{ch:prep}>>
	   wr.Flush()
  }
#+end_src
#+begin_export latex
We import the bufio writer.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"bufio"
#+end_src
#+begin_export latex
The header consists of a single line that starts with a hash followed
by the name of the query file and a key/value map to the subject
id. The pairs are written in the following manner :
\ty{subject\_id=subject\_file}.

We start by constructing the list of existing key/value pairs that are
contained inside the subjectID array. Then we print it out in a single
line.
#+end_export
#+begin_src go <<Write output header, Ch. \ref{ch:prep}>>=
  var pair []string
  seen := make(map[string]bool)
  //<<Check if key/value exists, Ch. \ref{ch:prep}>>
  //<<Write header, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
For every ID present in the subjectID array, we map it to the map of
subject IDs previously contructed and if its the first time it appears
we append it to the list of key-values to print. For more readability
we convert the subject ID to 1-based list. At the end we sort the
pairs.
#+end_export
#+begin_src go <<Check if key/value exists, Ch. \ref{ch:prep}>>=
  for _, ID := range id {
	  if seq, exists := sID[ID]; exists {
		  if !seen[seq] {
			  pair = append(pair,
				  fmt.Sprintf("%d=%s",
					  ID+1,seq))
			  seen[seq] = true
		  }
	  }
  }

  sort.Strings(pair)
#+end_src
#+begin_export latex
Then we write the header to the file. First we print the hash
character denoting the header and the name of the query file.  Then we
iterate over the pairs of subject IDs and filenames and print them
into the same line.

At the end we print a newline.
#+end_export
#+begin_src go <<Write header, Ch. \ref{ch:prep}>>=
  fmt.Fprintf(wr, "#%s\t", query)
  fmt.Fprintf(wr, "%s", strings.Join(pair, "\t"))
  fmt.Fprintf(wr,"\n")
#+end_src
#+begin_export latex
We import strings.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"strings"
#+end_src
#+begin_export latex
Then we write the matches and corresponding subject IDs where they
were found. For this we iterate over the length of the matchesLength,
and for each position print the corresponding matche length and the
subject ID. We also convert the subjectID into a 1-based list to match
the header.
#+end_export
#+begin_src go <<Write matches and subject ID, Ch. \ref{ch:prep}>>=
  for j := 0; j < len(ml); j++ {
	  fmt.Fprintf(wr, "%d\t%d\n",
		  ml[j],
		  id[j]+1)
  }
#+end_src
#+begin_export latex
We have now finished \ty{prepAlfy}, which still needs to be tested.

\section{Testing}
The program to test \ty{prepAlfy} has hooks for imports and the
testing logic.
#+end_export
#+begin_src go <<prepAlfy_test.go>>=
  package main

  import(
	  //<<Testing imports, Ch. \ref{ch:prep}>>
  )
  func TestPrepAlfy(t *testing.T) {
	  //<<Testing, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We import testing.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
  "testing"
#+end_src
#+begin_export latex
We construct a set of tests and iterate over them.
#+end_export
#+begin_src go <<Testing, Ch. \ref{ch:prep}>>=
  var tests []*exec.Cmd
	  //<<Construct tests, Ch. \ref{ch:prep}>>
  for _, test := range tests {
	  //<<Run test, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We import exec.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
"os/exec"
#+end_src
#+begin_export latex
In the first test, we test the program with the default settings. For
that we set the queries and the subjects.
#+end_export
#+begin_src go <<Construct tests, Ch. \ref{ch:prep}>>=
  p := "./prepAlfy"
  q := "query"
  s := "subject"
  test := exec.Command(p, "-q", q, "-s", s)
  tests = append(tests, test)
#+end_src
#+begin_export latex
In a second test, we evaluate the number of threads by using a single
thread.
#+end_export
#+begin_src go <<Construct tests, Ch. \ref{ch:prep}>>=
  test = exec.Command(p, "-q", q, "-s", s, "-t", "1")
  tests = append(tests, test)
#+end_src
#+begin_export latex
We run a test and compare the results with the one we want stored in
the file r1.txt.
#+end_export
#+begin_src go <<Run test, Ch. \ref{ch:prep}>>=
  //<<Testing read output, Ch. \ref{ch:prep}>>
  //<<Testing read want, Ch. \ref{ch:prep}>>
  if !bytes.Equal(get,want) {
	  t.Errorf("get:\n%s\nwant:\n%s\n", get, want)
  }
#+end_src
#+begin_export latex
We read the output from \ty{prepAlfy}.
#+end_export
#+begin_src go <<Testing read output, Ch. \ref{ch:prep}>>=
  get, err := test.Output()
  if err != nil {
	  t.Error(err)
  }
#+end_src
#+begin_export latex
We read the contents of the r1.txt file.
#+end_export
#+begin_src go <<Testing read want, Ch. \ref{ch:prep}>>=
  f := "r1.txt"
  want, err := os.ReadFile(f)
  if err != nil {
	  t.Error(err)
  }
#+end_src
#+begin_export latex
We import os and bytes.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
  "os"
  "bytes"
#+end_src
