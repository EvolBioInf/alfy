#+begin_export latex
\section{Introduction}
The program \ty{prepAlfy} takes as input a directory of query fasta
files and a directory of subject fasta files. \ty{prepAlfy} will then
calculate the match lengths between the queries and subjects. The
output file is a two column table consisting of the query name,
followed by pairs of match lengths and sequence identifiers.
#+end_export
#+begin_export latex
\section{Implementation}

The outline for \ty{preAlfy} contains hooks for imports,
types, functions and the logic of the main function.
#+end_export
#+begin_src go <<prepAlfy.go>>=
  package main

  import (
	  //<<Imports, Ch. \ref{ch:prep}>>
  )
  //<<Types, Ch. \ref{ch:prep}>>
  //<<Functions, Ch. \ref{ch:prep}>>
  func main() {	 
	  //<<Main function, Ch. \ref{ch:prep}>>	
  }
#+end_src
#+begin_export latex
In the main function we prepare the error handling, declare the
options, set the usage, parse the options and construct the output
table.
#+end_export
#+begin_src go <<Main function, Ch. \ref{ch:prep}>>=
  //<<Prepare error handling, Ch. \ref{ch:prep}>>
  //<<Declare options, Ch. \ref{ch:prep}>>
  //<<Set usage, Ch. \ref{ch:prep}>>
  //<<Parse options, Ch. \ref{ch:prep}>>
  //<<Construct alfy input table, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
We take care of error handling by calling the function
\ty{PrepareErrorMessages} from the \ty{util} package.
#+end_export
#+begin_src go <<Prepare error handling, Ch. \ref{ch:prep}>>=
util.PrepareErrorMessages("prepAlfy")
#+end_src
#+begin_export latex
We import util.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/alfy/util"
#+end_src
#+begin_export latex
We declare options for printing the version, picking the query and
subject directories and the number of threads. By default this
corresponds to the number of logical CPUs available on the system. We
also allow the user to print subject names instead of identifiers.
#+end_export
#+begin_src go <<Declare options, Ch. \ref{ch:prep}>>=
  optV := flag.Bool("v", false, "version")
  optQ := flag.String("q", "", "query directory")
  optS := flag.String("s", "", "subject directory")
  ncpu := runtime.NumCPU()
  optT := flag.Int("t", ncpu , "number of threads")
  optN := flag.Bool("n", false, "print subject names " +
	  "(default identifiers)")
#+end_src
#+begin_export latex
We import flag and runtime.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "flag"
  "runtime"
#+end_src
#+begin_export latex
The usage consists of three parts, the actual usage message, an
explanation of the purpose of \ty{prepAlfy}, and an example command.
#+end_export
#+begin_src go <<Set usage, Ch. \ref{ch:prep}>>=
  u :="prepAlfy -q <queryDir> -s <subjectDir>"
  p :="Prepare alfy input"
  e :="prepAlfy -q queries/ -s subjects/"
  clio.Usage(u,p,e)
#+end_src
#+begin_export latex
We import clio.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/clio"
#+end_src
#+begin_export latex
We parse the options and respond to the version option, -v, the two
directory options, \ty{-q} and \ty{-s}, and the number of threads,
\ty{-t}.
#+end_export
#+begin_src go <<Parse options, Ch. \ref{ch:prep}>>=
  flag.Parse()
  //<<Respond to \ty{-v}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-q}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-s}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-t}, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
If the user requested the version, we call the utility function
\ty{Version} with the program name.
#+end_export
#+begin_src go <<Respond to \ty{-v}, Ch. \ref{ch:prep}>>=
  if *optV {
	  util.Version("prepAlfy")
  }
#+end_src
#+begin_export latex
If the user didn't set a query directory, we exit with a friendly
message.
#+end_export
#+begin_src go <<Respond to \ty{-q}, Ch. \ref{ch:prep}>>=
  if *optQ == "" {
	  m := "please provide a directory " +
		  "of query sequences"
	  fmt.Fprintf(os.Stderr,"%s\n",m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
We import fmt and os.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "fmt"
  "os"
#+end_src
#+begin_export latex
We also exit with a friendly message if the subject directory was not
set.
#+end_export
#+begin_src go <<Respond to \ty{-s}, Ch. \ref{ch:prep}>>=
  if *optS == "" {
	  m := "please provide a directory " +
		  "of subject sequences"
	  fmt.Fprintf(os.Stderr,"%s\n",m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
If the user set fewer than one thread, we throw a fatal error.
#+end_export
#+begin_src go <<Respond to \ty{-t}, Ch. \ref{ch:prep}>>=
  if *optT <= 0 {
	  log.Fatalf("Can't set %d threads.", *optT)
  }
#+end_src
#+begin_export latex
We import log.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "log"
#+end_src
#+begin_export latex
To construct the alfy input table, we read the names of the queries
and subjects and make sure there are no overlaps. This is done by
storing the names of the files in maps. Since maps aren't ordered, we
also sort the names beforehand.

Then we construct the elements required for the output table: the
query name, the subject ID-key pairing and the matches lengths.
#+end_export
#+begin_src go <<Construct alfy input table, Ch. \ref{ch:prep}>>=
  //<<Read names of queries and subjects, Ch. \ref{ch:prep}>>
  //<<Sort names of queries and subjects, Ch. \ref{ch:prep}>>
  //<<Do queries and subjects overlap, Ch. \ref{ch:prep}>>
  //<<Write match factors, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
The reading of the queries and subjects is done by the function
readDir. If either of the directories are empty, we exit and warn the
user.
#+end_export
#+begin_src go <<Read names of queries and subjects, Ch. \ref{ch:prep}>>=
  //<<Read names of queries, Ch. \ref{ch:prep}>>
  //<<Read names of subjects, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
We start by reading the query directory.
#+end_export
#+begin_src go <<Read names of queries, Ch. \ref{ch:prep}>>=
  queries := readDir(*optQ)
  if len(queries) == 0 {
	  fmt.Fprintf(os.Stderr, "%s is empty\n", *optQ)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
We then read the subject directory.
#+end_export
#+begin_src go <<Read names of subjects, Ch. \ref{ch:prep}>>=
  subjects := readDir(*optS)
  if len(subjects) == 0 {
	  fmt.Fprintf(os.Stderr, "%s is empty\n", *optS)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
The function \ty{readDir} takes as input the name of a directory and
returns a map of the names of the sequence files within the
directory. We check each directory entry before storing its name.
#+end_export
#+begin_src go <<Functions, Ch. \ref{ch:prep}>>=
  func readDir(dir string) map[string]bool {
	  dirEntries, err := os.ReadDir(dir)
	  util.Check(err)
	  names := make(map[string]bool)
	  for _, dirEntry := range dirEntries {
		  //<<Check directory entry, Ch. \ref{ch:prep}>>
		  names[dirEntry.Name()] = true
	  }
	  return names
  }
#+end_src
#+begin_export latex
Directory entries should consists of files and not subdirectories. The
files themselves should be nucleotide FASTA files.
#+end_export
#+begin_src go <<Check directory entry, Ch. \ref{ch:prep}>>=
  //<<Check if entry is a file, Ch. \ref{ch:prep}>>
  //<<Check if entry is nucleotide FASTA, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
If the entry is a subdirectory, we skip it and warn the user.
#+end_export
#+begin_src go <<Check if entry is a file, Ch. \ref{ch:prep}>>=
  if dirEntry.IsDir() {
	  p := dir + "/" + dirEntry.Name()
	  fmt.Fprintf(os.Stderr,
		  "Skipping subdirectory %s\n", p)
	  continue
  }
#+end_src
#+begin_export latex
Then we check if the entry is a nucleotide FASTA format. There are
currently five filenames extensions that can refer to nucleotide FASTA
files listed on
Wikipedia\footnote{\ty{https://en.wikipedia.org/wiki/FASTA\_format}}:
fasta, fna, ffn, frn and fa. If the file has a different extension, or
none, we skip it and warn the user.
#+end_export
#+begin_src go <<Check if entry is nucleotide FASTA, Ch. \ref{ch:prep}>>=
  ext := filepath.Ext(dirEntry.Name())
  if ext != ".fasta" && ext != ".fna" &&
	  ext != ".ffn" && ext != ".fnr" &&
	  ext != ".fa" {
	  m := "%s doesnt have the extension of a " +
		  "nucleotide FASTA file." +
		  "Skipping it \n"
	  p := dir + "/" + dirEntry.Name()
	  fmt.Fprintf(os.Stderr, m, p)
	  continue
  }
#+end_src
#+begin_export latex
We import filepath.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "path/filepath"
#+end_src
#+begin_export latex
We store the names of the queries in slices and sort them. Then we
store the subject IDs.
#+end_export
#+begin_src go <<Sort names of queries and subjects, Ch. \ref{ch:prep}>>=
  var queryNames, subjectNames []string
  var subjectIDs []int
  //<<Sort names of queries, Ch. \ref{ch:prep}>>
  //<<Sort names of subjects, Ch. \ref{ch:prep}>>
  //<<Store subject IDs, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
First we sort the queries.
#+end_export
#+begin_src go <<Sort names of queries, Ch. \ref{ch:prep}>>=
  for query := range queries {
	  queryNames = append(queryNames, query)
  }
  sort.Strings(queryNames)
#+end_src
#+begin_export latex
We import sort.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "sort"
#+end_src
#+begin_export latex
Then we do the same for the subjects.
#+end_export
#+begin_src go <<Sort names of subjects, Ch. \ref{ch:prep}>>=
  for subject := range subjects {
	  subjectNames = append(subjectNames, subject)
  }
  sort.Strings(subjectNames)
#+end_src
#+begin_export latex
The subject IDs are simply the indexes in the slice of subject names.
#+end_export
#+begin_src go <<Store subject IDs, Ch. \ref{ch:prep}>>=
  for i, _ := range subjectNames {
	  subjectIDs = append(subjectIDs, i)
  }
#+end_src
#+begin_export latex
We then check if there is an overlap in the queries and subject
files. To do so, we search for pairs of repeated file names and exit
with a message if we find one.
#+end_export
#+begin_src go <<Do queries and subjects overlap, Ch. \ref{ch:prep}>>=           
  for _, query := range queryNames {
	  if subjects[query] {
		  m := "Found %s%s and %s%s." +
			  "Please ensure queries and " +
			  "subjects do not " +
			  "overlap."
		  fmt.Fprintf(os.Stderr, m, *optQ, query,
			  *optS, query)
		  os.Exit(1)
	  }
  }
#+end_src
#+begin_export latex
\subsection{Write match factors}

We now move on to the calculation of the match factors. This is main
algorithm of \ty{prepAlfy}. Since subject sequences may be composed of
several short contigs, it is more efficient to concatenate shorter
contigs into one larger sequence. To do so, we first determine the
maximum subject length. This value will be later using during the
streaming step where we stream the queries against the subjects. Since
the maximum length only needs to be calculated once, we do it before
we iterate over the query sequences. Each query is to be streamed
against the subjects to calculate the matches lengths. This process is
distributed among goroutines. The distribution of the sequences across
the goroutines will depend on the maximum subject length previously
calculated.

The subjects will be streamed against each query at a time. After
streaming the subjects the resulting match lengths are merged while
taking note of the subject they were found in. We then calculate the
enhanced suffix array, or esa, for each one of the subject files. At
the end the pair match length and subject ID is written out to a file.
#+end_export
#+begin_src go <<Write match factors, Ch. \ref{ch:prep}>>=
  //<<Find maximum subject length, Ch. \ref{ch:prep}>>
  //<<Calculate enhanced suffix arrays, Ch. \ref{ch:prep}>>
  //<<Read query sequences, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
To find the maximal subject length, we iterate over the subject files
and for each file iterate over the sequences it contains to find the
longest subject file.
#+end_export
#+begin_src go <<Find maximum subject length, Ch. \ref{ch:prep}>>=
  msl := -1
  for _, subject := range subjectNames {
	  p := *optS + "/" + subject
	  f, err := os.Open(p)
	  util.Check(err)
	  //<<Iterate over subjects, Ch. \ref{ch:prep}>>
	  f.Close()
  }
#+end_src
#+begin_export latex
We use a scanner to iterate over the sequences in the current neighbor file.
#+end_export
#+begin_src go <<Iterate over subjects, Ch. \ref{ch:prep}>>=
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  l := len(sc.Sequence().Data())
	  if l > msl {
		  msl = l
	  }
  }
#+end_src
#+begin_export latex
We import fasta
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"github.com/evolbioinf/fasta"
#+end_src
#+begin_export latex
We then calculate the enhanced suffix arrays for all subject
files. First we create a map between the attributed numerical subject
ID and the subject filename. We also create a variable that will hold
the calculated esas. Then we calculate the esa. This is done only once
for all subject files. For this we read the subject files, and map the
subject file name to a numerical ID.
#+end_export
#+begin_src go <<Calculate enhanced suffix arrays, Ch. \ref{ch:prep}>>= 
  sID := make(map[int]string)
  esas := make([]*esa.Esa,len(subjectNames))
  for i, subject := range subjectNames {
	  sID[i] = subject
	  p := *optS + "/" + subject
	  f, err := os.Open(p)
	  util.Check(err)
	  //<<Read subject file, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
While reading the subject file we check the length of the contig. If
it is shorter than the maximum subject length calculated above we
concatenate it. The esa is calculated on the concatenated sequences.
#+end_export
#+begin_src go <<Read subject file, Ch. \ref{ch:prep}>>=
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  s := sc.Sequence()
	  d := s.Data()
	  h := []byte(s.Header())
	  for len(d) < msl && sc.ScanSequence() {
		  s = sc.Sequence()
		  h = append(h, '|')
		  h = append(h, []byte(s.Header())...)
		  d = append(d, s.Data()...)
	  }
	  //<<Calculate enhanced suffix array, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We calculate the enhanced suffix array or \ty{esa} of the subject
sequences.
#+end_export
#+begin_src go <<Calculate enhanced suffix array, Ch. \ref{ch:prep}>>=
  d = bytes.ToUpper(d)
  esas[i] = esa.MakeEsa(d)
#+end_src
#+begin_export latex
We import esa.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/esa"
#+end_src
#+begin_export latex
We then start reading the query sequences. Here we iterate over the
queryNames which have been ordered. This ensures we read the query
files in the same order everytime. For each sequence we calculate the
forward and the reverse strands, and we remove the masking information
from the sequence. Then we iterate over their sequences and stream the
subjects against it. After streaming all the subjects we update the
final matches lengths and write it to a file.
#+end_export
#+begin_src go <<Read query sequences, Ch. \ref{ch:prep}>>=
  for _, query := range queryNames {
	  //<<Store forward and reverse strand, Ch. \ref{ch:prep}>>
	  //<<Remove masking and reverse sequence, Ch. \ref{ch:prep}>>
	  //<<Distribute streaming across goroutines, Ch. \ref{ch:prep}>>
	  //<<Merge matches results, Ch. \ref{ch:prep}>>
	  //<<Write matches to stdout, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We read the query files and we store the forward and the reverse
complement of the sequence. We close the file at the end.
#+end_export
#+begin_src go <<Store forward and reverse strand, Ch. \ref{ch:prep}>>= 
  p := *optQ + "/" + query
  f, err := os.Open(p)
  util.Check(err)
  var querySeqs, revQuerySeqs []*fasta.Sequence
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  s := sc.Sequence()
	  querySeqs = append(querySeqs, s)
	  s = fasta.NewSequence(s.Header(), s.Data())
	  s.ReverseComplement()
	  revQuerySeqs = append(revQuerySeqs, s)
  }
  f.Close()
#+end_src
#+begin_export latex
Since we are interested in exact matching we convert all nucleotides
to uppercase, removing the masking information from the sequence. Both
in the forward and in the reverse strand.
#+end_export
#+begin_src go <<Remove masking and reverse sequence, Ch. \ref{ch:prep}>>=
  for i, querySeq := range querySeqs {
	  h := querySeq.Header()
	  d := bytes.ToUpper(querySeq.Data())
	  querySeqs[i] = fasta.NewSequence(h, d)
	  h = revQuerySeqs[i].Header()
	  d = bytes.ToUpper(revQuerySeqs[i].Data())
	  revQuerySeqs[i] = fasta.NewSequence(h, d)
  }
#+end_src
#+begin_export latex
We import byte.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"bytes"
#+end_src
#+begin_export latex
Since streaming is the rate limiting step, we distribute the streaming
of the subjects across goroutines. For this we start by constructing a
structure to hold both the match lengths and the subjects ID. This
ensures both values are passed on together. The matches length is a
two dimensional slice, while the subjects IDs is a three dimensional
slice to allow multiple IDs per match length. Additionally we add a
third variable, id, to hold the order the matches were calculated.
#+end_export
#+begin_src go <<Types, Ch. \ref{ch:prep}>>=
  type Matches struct {
	  matchLengths [][]int
	  subjectID [][][]int
	  id int
  }
#+end_src
#+begin_export latex
We distribute the streaming work along sets of neighbors files. Thus,
we split the slice of esas into packages of size n. We then
concurrently stream the query against the sets of calculated esas. The
concurrency pattern is based on \ty{sync.WaitGroup}, and is built from
a channel and a \ty{WaitGroup}, both of which we initialize outside of
the loop from which the goroutines are started. Inside the loop we
increment the counter of working goroutines and construct the
goroutine for streaming. After the loop we construct the closer and
the iterator that belong to the \ty{WaitGroup} concurrency pattern.
#+end_export
#+begin_src go <<Distribute streaming across goroutines, Ch. \ref{ch:prep}>>=
  EsaSets := make([][]*esa.Esa, 0)
  subjectIDsets := make([][]int, 0)
  //<<Split esas across threads, Ch. \ref{ch:prep}>>
  matchesSets := make(chan Matches)
  var wg sync.WaitGroup
  for i, Esas := range EsaSets{
	  subjectIDs := subjectIDsets[i]
	  wg.Add(1)
	  //<<Goroutine for streaming, Ch. \ref{ch:prep}>>
  }
  //<<Streaming closer, Ch. \ref{ch:prep}>>
  //<<Streaming iterator, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
We import sync.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"sync"
#+end_src
#+begin_export latex
The esas are split into several sets such that $\ell=n/T$, where $n$
is the number of sets and $T$ is the number of threads. We also ensure
that the last of the slices does not extend beyond the length of the
slice of esas.
#+end_export
#+begin_src go <<Split esas across threads, Ch. \ref{ch:prep}>>=
  n := len(subjectNames)
  length := int(math.Ceil(float64(n)/float64(*optT)))
  start := 0
  end := length
  for start < n {
	  EsaSets = append(EsaSets,
		  esas[start:end])
	  subjectIDsets = append(subjectIDsets,
		  subjectIDs[start:end])
	  start = end
	  end += length
	  if end > n {
		  end = n
	  }
  }
#+end_src
#+begin_export latex
We import math.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"math"
#+end_src
#+begin_export latex
We pass the current set of esas and subject IDs to the
goroutine. Inside it we defer its closure. We add a counter for the
matches id, and we also declare and allocate space for holding slices
of matches lengths and subject IDs. Then we stream the query against
the esas and pass the matches along the channel.
#+end_export
#+begin_src go <<Goroutine for streaming, Ch. \ref{ch:prep}>>=
  go func(Esas []*esa.Esa, subjectIDs []int){
	  defer wg.Done()
	  var matches Matches
	  matches.id = i
	  //<<Allocate space for match lengths and subject ID, Ch. \ref{ch:prep}>>
	  //<<Stream query against esas, Ch. \ref{ch:prep}>>
	  matchesSets <- matches
  }(Esas, subjectIDs)
#+end_src
#+begin_export latex
Let $n$ be the number of sequences that make up the current query. We
allocate a slice of $n$ slices of match lengths and a separate slice
of $n$ slices of subject IDs. Then we iterate over the query sequences
and allocate the individual slices of match lengths and subject IDs,
each the same length as the respective sequence. We then do a third
and final allocation for the last dimension of the subject IDs. The
innermost slice is initially set to zero since it will varies between
1 and the number of subjects.
#+end_export
#+begin_src go <<Allocate space for match lengths and subject ID, Ch. \ref{ch:prep}>>=
  n :=len(querySeqs)
  matches.matchLengths = make([][]int,n)
  matches.subjectID = make([][][]int,n)
  for i, querySeq := range querySeqs {
	  m := len(querySeq.Data())
	  matches.matchLengths[i] = make([]int,m)
	  matches.subjectID[i] = make([][]int,m)
   	for j := range matches.subjectID[i] {
   		matches.subjectID[i][j] = make([]int,1)
   	}
  }
#+end_src
#+begin_export latex
To stream the query against the subject, we iterate over the sequences
that make up the query. For each sequence we stream the forward and
reverse strand.
#+end_export
#+begin_src go <<Stream query against esas, Ch. \ref{ch:prep}>>=
  for i, querySeq := range querySeqs {
	  //<<Stream forward strand, Ch. \ref{ch:prep}>>
	  //<<Stream reverse strand, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We stream the forward strand against the set of esas in the goroutine.
#+end_export
#+begin_src go <<Stream forward strand, Ch. \ref{ch:prep}>>=
  f := querySeq.Data()
  rev := false
  for j, esa := range Esas {
	  mat.UpdateMatchLengths(f, esa, subjectIDs[j],
		  matches.matchLengths[i],
		  matches.subjectID[i],
		  rev)
  }
#+end_src
#+begin_export latex
We repeat the streaming for the reverse strand.
#+end_export
#+begin_src go <<Stream reverse strand, Ch. \ref{ch:prep}>>=
  r := revQuerySeqs[i].Data()
  rev = true
  for j, esa := range Esas {
	  mat.UpdateMatchLengths(r, esa, subjectIDs[j],
		  matches.matchLengths[i],
		  matches.subjectID[i],
		  rev)
  }
#+end_src
#+begin_export latex
We import the mat package.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/alfy/mat"
#+end_src
#+begin_export latex
We reach the end of the goroutine. In the closer we wait until all
working goroutines have finished and then close the channel.
#+end_export
#+begin_src go <<Streaming closer, Ch. \ref{ch:prep}>>=
  go func() {
	  wg.Wait()
	  close(matchesSets)
  }()
#+end_src
#+begin_export latex
In the iterator we pack the matches structure with the results from
the channel. To ensure that we summarize the matches in the same
order, we sort the matches with the match id counter started inside
the goroutine. Then we iterate over the sorted matches and the results
from the matches lengths. The match lengths are a two dimensional
slice, while the subject IDs are three dimensional slices.
#+end_export
#+begin_src go <<Streaming iterator, Ch. \ref{ch:prep}>>=
  matchLengths := make([][]int,0)
  subjectIDs := make([][][]int,0)
  for _, qs := range querySeqs {
	  //<<Construct master sets, Ch. \ref{ch:prep}>>
  }
  //<<Sort matches sets, Ch. \ref{ch:prep}>>
  for _, match := range msSlice {
	  //<<Store match lengths, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We construct the master sets for the match lengths and the subject
IDs. The master sets consists of slices of matches lengths over the
length of the query sequences.
#+end_export
#+begin_src go <<Construct master sets, Ch. \ref{ch:prep}>>=
  n := len(qs.Data())
  ml := make([]int, n)
  matchLengths = append(matchLengths, ml)
  mls := make([][]int, n)
  subjectIDs = append(subjectIDs,mls)
#+end_src
#+begin_export latex
We then sort the the machest sets to ensure that the matches are all
summarized in the same order. This is because in some situations the
subject IDs can be order dependent. To sort the matches, we first
contruct a slice of matches and then sort it.
#+end_export
#+begin_src go <<Sort matches sets, Ch. \ref{ch:prep}>>=
  //<<Construct slice of matches, Ch. \ref{ch:prep}>>
  //<<Sort slice of matches, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
To construct the slice of matches we iterate over the range of the
sets of matches and construct the slice.
#+end_export
#+begin_src go <<Construct slice of matches, Ch. \ref{ch:prep}>>=
  msSlice := []Matches{}
  for matchesSet := range matchesSets {
	  msSlice = append(msSlice, matchesSet)
  }
#+end_src
#+begin_export latex
Then we sort the slice. Since we are sorting a type Matches we write
our own sorting function. This will sort the matches id in ascending
order.
#+end_export
#+begin_src go <<Sort slice of matches, Ch. \ref{ch:prep}>>=
  slices.SortFunc(msSlice, func(a, b Matches) int {
	  if a.id < b.id {
		  return -1
	  } else if a.id == b.id {
		  return 0
	  }
	  return 1
  })
#+end_src
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"slices"
#+end_src
#+begin_export latex
We store the longer of the matches for each position and the
corresponding slice of subject IDs. If we did not store a new value,
we check whether the current match has a length of 0. If yes, we also
store the corresponding subject IDs.
#+end_export
#+begin_src go <<Store match lengths, Ch. \ref{ch:prep}>>=
  for i, lengths := range match.matchLengths {
	  for j, length := range lengths {
		  if matchLengths[i][j] < length {
			  matchLengths[i][j] = length
			  subjectIDs[i][j] =
				  match.subjectID[i][j]
		  } else if matchLengths[i][j] == 0 {
			  matchLengths[i][j] = length
			  subjectIDs[i][j] =
				  match.subjectID[i][j]
		  }
	  }
  }
#+end_src
#+begin_export latex
We then merge the matches from the several routines. We go through
every position and note down the length of the longest match. We also
initialize a variable for keeping track of the subject ID from the
previous iteration. In the first iteration this will be nil thus we
also keep track of whether we are at the first loop iteration. Then we
check whether we are standing before a new match, and we update the
loop variables.
#+end_export
#+begin_src go <<Merge matches results, Ch. \ref{ch:prep}>>=
  for i, ml := range matchLengths {
	  l := 0
	  var id []int
	  var previous []int
	  first := true
	  for j := 0; j < len(ml); j++ {
		  //<<Is it a new match?, Ch. \ref{ch:prep}>>
		  //<<Update variables, Ch. \ref{ch:prep}>>
	  }
  }
#+end_src
#+begin_export latex
New matches occur if the match length is longer than the length of the
previous match. In that case, we update both the match length and the
subject ID. It may happen that the match length of the previous and
the current position are identical but the subject IDs change. This
means we are also standing before a new match. In this case we also
update the subject ID.
#+end_export
#+begin_src go <<Is it a new match?, Ch. \ref{ch:prep}>>=
		    if ml[j] > l {
			    l = ml[j]
			    id = subjectIDs[i][j]
		    } else if !first  &&
			    slices.Compare(subjectIDs[i][j],previous) != 0 {
			    id = subjectIDs[i][j]
		    }

#+end_src
#+begin_export latex
After checking for new matches we store the current match length and
subject ID. We decrease the stored length by 1 and ensure that the new
length is not negative. Finally we store the id of the current
iteration.
#+end_export
#+begin_src go <<Update variables, Ch. \ref{ch:prep}>>=
		    ml[j] = l
		    subjectIDs[i][j] = id
		    l--
		    if l < 0{
			    l = 0
		    }
		    first = false
		    previous = id

#+end_src
#+begin_export latex
After finishing streaming the subject files and updating the arrays of
matches lengths and IDs we write them to standard out. We start by
printing out the query filename.
#+end_export
#+begin_src go <<Write matches to stdout, Ch. \ref{ch:prep}>>=
  fmt.Printf("#%s\n", query)
  for i, querySeq := range querySeqs {
	  ml := matchLengths[i]
	  ids := subjectIDs[i]
	   //<<Write output header, Ch. \ref{ch:prep}>>
	  //<<Write matches and subjects, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
The header consists of a single line that starts with a hash followed
by the name of the query file and a key/value map to the subject
id. Unless the user requested subject names, the query name is
followed by a legend consisting of pairs of subject ID and subject
file linked by ``='', that is,
\begin{verbatim}
subject_id=subject_file
\end{verbatim}
To construct the legend, we determine the list of subject IDs that are
present in the results and then print it out.
#+end_export
#+begin_src go <<Write output header, Ch. \ref{ch:prep}>>=
  fmt.Printf(">%s",querySeq.Header())
  if !*optN {
	  //<<Determine subjects seen, Ch. \ref{ch:prep}>>
	  //<<Print subjects seen, Ch. \ref{ch:prep}>>
  }
  fmt.Printf("\n")
#+end_src
#+begin_export latex
The subjects seen are the distinct subject IDs annotated for the
current query. To find these distinct IDs, we create a map to mark
subject IDs, \ty{marked}, and a slice to store them in,
\ty{seen}. Then we iterate over the range of ids, and if the id has
not been marked, we do so. At the end we append all the seen ids. To
stabilize our output, we sort the subject IDs we've seen.
#+end_export
#+begin_src go <<Determine subjects seen, Ch. \ref{ch:prep}>>=
  marked := make(map[int]bool)
  seen := []int{}
  for _, id := range ids {
	  for p := range id {
		  marked[p] = true
	  }
  }
  for i := range marked {
	  seen = append(seen,i)
  }
  slices.Sort(seen)
#+end_src
#+begin_export latex
We print the legend of subject IDs and names. IDs are printed in
1-based notation.
#+end_export
#+begin_src go <<Print subjects seen, Ch. \ref{ch:prep}>>=
  for _, id := range seen {
	  fmt.Printf(" %d=%s", id+1, subjectNames[id])
  }
#+end_src
#+begin_export latex
Then we write the matches and corresponding subjects where they were
found. For this we iterate over the match lengths, and print each one
together with the subject name or ID in a two-column table. Subject
IDs are converted to 1-based values to match the header.
#+end_export
#+begin_src go <<Write matches and subjects, Ch. \ref{ch:prep}>>=
  for j := 0; j < len(ml); j++ {
	  str := make([]string, len(ids[j]))
	  if *optN {
		  for i, val := range ids[j] {
			  name := strconv.Itoa(val+1)
			  name = subjectNames[val]
			  str[i] = fmt.Sprintf("%v",name)
		  }
	  } else {
		  for i, val := range ids[j] {
			  str[i] = fmt.Sprintf("%d",val+1)
		  }
	  }
	  fmt.Printf("%d\t%s\n", ml[j], strings.Join(str,","))
  }
#+end_src
#+begin_export latex
We import \ty{strconv} and \ty{strings}.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "strconv"
  "strings"
#+end_src
#+begin_export latex
We have now finished \ty{prepAlfy}, which still needs to be tested.

\section{Testing}
The program to test \ty{prepAlfy} has hooks for imports and the
testing logic.
#+end_export
#+begin_src go <<prepAlfy_test.go>>=
  package main

  import(
	  //<<Testing imports, Ch. \ref{ch:prep}>>
  )
  func TestPrepAlfy(t *testing.T) {
	  //<<Testing, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We import testing.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
  "testing"
#+end_src
#+begin_export latex
We construct a set of tests and iterate over them.
#+end_export
#+begin_src go <<Testing, Ch. \ref{ch:prep}>>=
  var tests []*exec.Cmd
	  //<<Construct tests, Ch. \ref{ch:prep}>>
  for i, test := range tests {
	  //<<Run test, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We import exec.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
"os/exec"
#+end_src
#+begin_export latex
In the first test, we test the program with the default settings. For
that we set the queries and the subjects.
#+end_export
#+begin_src go <<Construct tests, Ch. \ref{ch:prep}>>=
  p := "./prepAlfy"
  q := "query"
  s := "subject"
  test := exec.Command(p, "-q", q, "-s", s)
  tests = append(tests, test)
#+end_src
#+begin_export latex
In a second test, we test setting the number of threads by using a
single thread.
#+end_export
#+begin_src go <<Construct tests, Ch. \ref{ch:prep}>>=
  test = exec.Command(p, "-q", q, "-s", s, "-t", "1")
  tests = append(tests, test)
#+end_src
#+begin_export latex
In our third and last test, we use subject names instead of IDs.
#+end_export
#+begin_src go <<Construct tests, Ch. \ref{ch:prep}>>=
  test = exec.Command(p, "-q", q, "-s", s, "-n")
  tests = append(tests, test)
#+end_src
#+begin_export latex
We run a test and compare the results we get with the results we want,
which are stored in files \ty{r1.txt}, \ty{r2.txt}, and so on.
#+end_export
#+begin_src go <<Run test, Ch. \ref{ch:prep}>>=
  //<<Testing read output, Ch. \ref{ch:prep}>>
  //<<Testing read want, Ch. \ref{ch:prep}>>
  if !bytes.Equal(get,want) {
	  t.Errorf("get:\n%s\nwant:\n%s\n", get, want)
  }
#+end_src
#+begin_export latex
We import \ty{bytes}.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
  "bytes"
#+end_src
#+begin_export latex
We read the output from \ty{prepAlfy}.
#+end_export
#+begin_src go <<Testing read output, Ch. \ref{ch:prep}>>=
  get, err := test.Output()
  if err != nil {
	  t.Error(err)
  }
#+end_src
#+begin_export latex
We read the contents of the appropriate results file, \verb+r*.txt+.
#+end_export
#+begin_src go <<Testing read want, Ch. \ref{ch:prep}>>=
  f := "r" + strconv.Itoa(i+1) + ".txt"
  want, err := os.ReadFile(f)
  if err != nil {
	  t.Error(err)
  }
#+end_src
#+begin_export latex
We import \ty{strconv} and \ty{os}.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
  "strconv"
  "os"
#+end_src
