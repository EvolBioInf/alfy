#+begin_export latex
\section{Introduction}
The program \ty{prepAlfy} takes as input a directory of query fasta
files and a directory of subject fasta files. \ty{prepAlfy} will then
calculate the match lengths between the queries and subjects. The
output file is a two column table consisting of the query name,
followed by pairs of match lengths and sequence identifiers.
#+end_export
#+begin_export latex
\section{Implementation}

The outline for \ty{preAlfy} contains hooks for imports, functions and
the logic of the main function.
#+end_export
#+begin_src go <<prepAlfy.go>>=
  package main

  import (
	  //<<Imports, Ch. \ref{ch:prep}>>
  )

  //<<Functions, Ch. \ref{ch:prep}>>
  func main() {	 
	  //<<Main function, Ch. \ref{ch:prep}>>	
  }
#+end_src
#+begin_export latex
In the main function we prepare the error handling, declare the
options, set the usage, parse the options and construct the index
tables.
#+end_export
#+begin_src go <<Main function, Ch. \ref{ch:prep}>>=
  //<<Prepare error handling, Ch. \ref{ch:prep}>>
  //<<Declare options, Ch. \ref{ch:prep}>>
  //<<Set usage, Ch. \ref{ch:prep}>>
  //<<Parse options, Ch. \ref{ch:prep}>>
  //<<Construct alfy input table, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
We take care of error handling by calling the function
\ty{PrepareErrorMessages} from the \ty{util} package.
#+end_export
#+begin_src go <<Prepare error handling, Ch. \ref{ch:prep}>>=
util.PrepareErrorMessages("prepAlfy")
#+end_src
#+begin_export latex
We declare options for printing the version, picking the query and
subject directories and the number of threads. By default this will
corresponds to the number of logical CPUs available on the system.
#+end_export
#+begin_src go <<Declare options, Ch. \ref{ch:prep}>>=
  optV := flag.Bool("v", false, "version")
  optQ := flag.String("q", "", "query directory")
  optS := flag.String("s", "", "subject directory")
  ncpu := runtime.NumCPU()
  optT := flag.Int("T", ncpu , "number of threads")
#+end_src
#+begin_export latex
We import flag and runtime.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "flag"
  "runtime"
#+end_src
#+begin_export latex
The usage constis of three parts, the actual usage message, an
explanation of the purpose of \ty{prepAlfy}, and an example command.
#+end_export
#+begin_src go <<Set usage, Ch. \ref{ch:prep}>>=
  u :="prepAlfy -q <queryDir> -s <subjectDir>"
  p :="Prepare alfy input"
  e :="prepAlfy -q queries/ -s subjects/"

  clio.Usage(u,p,e)
#+end_src
#+begin_export latex
We import clio.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/clio"
#+end_src
#+begin_export latex
We parse the options and respond to the version option, -v, the two
directory options, \ty{-q}, \ty{-s} and the number of threads \ty{-T}.
#+end_export
#+begin_src go <<Parse options, Ch. \ref{ch:prep}>>=
  flag.Parse()
  //<<Respond to \ty{-v}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-q}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-s}, Ch. \ref{ch:prep}>>
  //<<Respond to \ty{-T}, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
If the user requested the version, we call the utility function
\ty{Version} with the program name.
#+end_export
#+begin_src go <<Respond to \ty{-v}, Ch. \ref{ch:prep}>>=
  if *optV {
	  util.Version("prepAlfy")
  }
#+end_src
#+begin_export latex
We import util.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/alfy/util"
#+end_src
#+begin_export latex
If the user didn't set a query directory, we exit with a friendly
message.
#+end_export
#+begin_src go <<Respond to \ty{-q}, Ch. \ref{ch:prep}>>=
  if *optQ == "" {
	  m := "please provide a directory " +
		  "of query sequences"
	  fmt.Fprintf(os.Stderr,"%s\n",m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
We import fmt and os.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "fmt"
  "os"
#+end_src
#+begin_export latex
We also exit with a friendly message if the subject directory was not
set.
#+end_export
#+begin_src go <<Respond to \ty{-s}, Ch. \ref{ch:prep}>>=
  if *optS == "" {
	  m := "please provide a directory " +
		  "of subject sequences"
	  fmt.Fprintf(os.Stderr,"%s\n",m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
If the user set a negative number of threads, we throw a fatal error.
#+end_export
#+begin_src go <<Respond to \ty{-T}, Ch. \ref{ch:prep}>>=
  if *optT < 0 {
	  log.Fatalf("Can't set %d threads.", *optT)
  }
#+end_src
#+begin_export latex
We import log.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "log"
#+end_src
#+begin_export latex
To construct the alfy input table, we read the names of the queries
and subjects and make sure there are no overlaps. This is done by
storing the names of the files in maps. Since maps aren't ordered, we
also sort the names beforehand.

Then we construct the elements required for the output table: the
query name, the subject ID-key pairing and the matches lengths.
#+end_export
#+begin_src go <<Construct alfy input table, Ch. \ref{ch:prep}>>=
  //<<Read names of queries and subjects, Ch. \ref{ch:prep}>>
  //<<Sort names of queries and subjects, Ch. \ref{ch:prep}>>
  //<<Do queries and subjects overlap, Ch. \ref{ch:prep}>>
  //<<Write match factors, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
The reading of the queries and subjects is done by the function
readDir. If either of the directories are empty, we exit and warn the
user.
#+end_export
#+begin_src go <<Read names of queries and subjects, Ch. \ref{ch:prep}>>=
  queries := readDir(*optQ)

  if len(queries) == 0 {
	  fmt.Fprintf(os.Stderr, "%s is empty\n", *optQ)
	  os.Exit(1)
  }

  subjects := readDir(*optS)

  if len(subjects) == 0 {
	  fmt.Fprintf(os.Stderr, "%s is empty\n", *optS)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
The function \ty{readDir} takes as input the name of a directory and
returns a map of the names of the sequence files within the
directory. We check each directory entry before storing its name.
#+end_export
#+begin_src go <<Functions, Ch. \ref{ch:prep}>>=
  func readDir(dir string) map[string]bool {
	  dirEntries, err := os.ReadDir(dir)
	  util.Check(err)
	  names := make(map[string]bool)

	  for _, dirEntry := range dirEntries {
		  //<<Check directory entry, Ch. \ref{ch:prep}>>
		  names[dirEntry.Name()] = true
	  }
	  return names
  }
#+end_src
#+begin_export latex
Directory entries should consists of files and not subdirectories. The
files themselves should be nucleotide FASTA files.
#+end_export
#+begin_src go <<Check directory entry, Ch. \ref{ch:prep}>>=
  //<<Check if entry is a file, Ch. \ref{ch:prep}>>
  //<<Check if entry is nucleotide FASTA, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
If the entry is a subdirectory, we skip it and warn the user.
#+end_export
#+begin_src go <<Check if entry is a file, Ch. \ref{ch:prep}>>=
  if dirEntry.IsDir() {
	  p := dir + "/" + dirEntry.Name()
	  fmt.Fprintf(os.Stderr,
		  "Skipping subdirectory %s\n", p)
	  continue
  }
#+end_src
#+begin_export latex
Then we check if the entry is a nucleotide FASTA format. There are
currently five filenames extensions that can refer to nucleotide FASTA
files listed on
Wikipedia\footnote{\ty{https://en.wikipedia.org/wiki/FASTA\_format}}:
fasta, fna, ffn, frn and fa. If the file has a different extension, or
none, we skip it and warn the user.
#+end_export
#+begin_src go <<Check if entry is nucleotide FASTA, Ch. \ref{ch:prep}>>=
  ext := filepath.Ext(dirEntry.Name())

  if ext != ".fasta" && ext != ".fna" &&
	  ext != ".ffn" && ext != ".fnr" &&
	  ext != ".fa" {
	  m := "%s doesnt have the extension of a " +
		  "nucleotide FASTA file." +
		  "Skipping it \n"
	  p := dir + "/" + dirEntry.Name()
	  fmt.Fprintf(os.Stderr, m, p)
	  continue
  }
#+end_src
#+begin_export latex
We import filepath.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "path/filepath"
#+end_src
#+begin_export latex
We store the names of the neighbors and targets in slices and sort them.
#+end_export
#+begin_src go <<Sort names of queries and subjects, Ch. \ref{ch:prep}>>=
  var queryNames, subjectNames []string

  for query := range queries {
	  queryNames = append(queryNames, query)
  }

  sort.Strings(queryNames)

  for subject := range subjects {
	  subjectNames = append(subjectNames, subject)
  }

  sort.Strings(subjectNames)
#+end_src
#+begin_export latex
We import sort.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "sort"
#+end_src
#+begin_export latex
We then check if there is an overlap in the queries and subject
files. To do so, we search for pairs of repeated file names and exit
with a message if we find one.
#+end_export
#+begin_src go <<Do queries and subjects overlap, Ch. \ref{ch:prep}>>=           
  for _, query := range queryNames {
	  if subjects[query] {
		  m := "Found %s%s and %s%s." +
			  "Please ensure queries and " +
			  "subjects do not " +
			  "overlap."

		  fmt.Fprintf(os.Stderr, m, *optQ, query,
			  *optS, query)
		  os.Exit(1)
	  }
  }
#+end_src
#+begin_export latex
\subsection{Write match factors}

We now move on to the calculation of the match factors. This is main
algorithm of \ty{prepAlfy}. Since subjects sequences may be composed
by several short contigs, it is more efficient to concatenate shorter
contigs into a larger sequences. To do so, we first determine the
maximum subject length. This value will be later using during the
streaming step where we stream the queries against the subjects. Since
the maximum length only needs to be calculated once, we do it before
we iterate over the query sequences. Each query is to be streamed
against the subjects to calculate the matches lengths. This process is
distributed among goroutines. The distribution of the sequences across
the goroutines will depend on the maximum subject length previously
calculated.

The subjects will be streamed against each query at a time. After
streaming the subjects the resulting match lengths are merged while
taking note of the subject they were found in. At the end the pair
match length and subject ID is written out to a file.
#+end_export
#+begin_src go <<Write match factors, Ch. \ref{ch:prep}>>=
  //<<Find maximum subject length, Ch. \ref{ch:prep}>>
  //<<Read query sequences, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
To find the maximal subject length, we iterate over the subject files
and for each file iterate over the sequences it contains to find the
longest.
#+end_export
#+begin_src go <<Find maximum subject length, Ch. \ref{ch:prep}>>=
  msl := -1

  for _, subject := range subjectNames {
	  p := *optS + "/" + subject
	  f, err := os.Open(p)
	  util.Check(err)
	  //<<Iterate over subjects, Ch. \ref{ch:prep}>>
	  f.Close()
  }
#+end_src
#+begin_export latex
We use a scanner to iterate over the sequences in the current neighbor file.
#+end_export
#+begin_src go <<Iterate over subjects, Ch. \ref{ch:prep}>>=
  sc := fasta.NewScanner(f)

  for sc.ScanSequence() {
	  l := len(sc.Sequence().Data())
	  if l > msl {
		  msl = l
	  }
  }
#+end_src
#+begin_export latex
We import fasta
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"github.com/evolbioinf/fasta"
#+end_src
#+begin_export latex
We then start reading the query sequences. For each sequence we
calculate the forward and the reverse strands, and we remove the
masking information from the sequence. Then we iterate over their
sequences and stream the subjects against it. After streaming all the
subjects we update the final matches lengths and write it to a file.
#+end_export
#+begin_src go <<Read query sequences, Ch. \ref{ch:prep}>>=
  for query, _ := range queries {
	  //<<Store forward and reverse strand, Ch. \ref{ch:prep}>>
	  //<<Remove masking and reverse sequence, Ch. \ref{ch:prep}>>
	  //<<Allocate space for match lengths and subject ID, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We read the query files and we store the forward and the reverse
complement of the sequence. We close the file at the end.
#+end_export
#+begin_src go <<Store forward and reverse strand, Ch. \ref{ch:prep}>>= 
  p := *optQ + "/" + query
  f, err := os.Open(p)
  util.Check(err)

  var querySeqs, revQuerySeqs []*fasta.Sequence

  sc := fasta.NewScanner(f)

  for sc.ScanSequence() {
	  s := sc.Sequence()
	  querySeqs = append(querySeqs, s)
	  s = fasta.NewSequence(s.Header(), s.Data())
	  s.ReverseComplement()
	  revQuerySeqs = append(revQuerySeqs, s)
  }
  f.Close()
#+end_src
#+begin_export latex
Since we are interested in exact matching we convert all nucleotides
to uppercase, removing the masking information from the sequence. Both
in the forward and in the reverse strand.
#+end_export
#+begin_src go <<Remove masking and reverse sequence, Ch. \ref{ch:prep}>>=
  for i, querySeq := range querySeqs {
	  h := querySeq.Header()
	  d := bytes.ToUpper(querySeq.Data())
	  querySeqs[i] = fasta.NewSequence(h, d)

	  h = revQuerySeqs[i].Header()
	  d = bytes.ToUpper(revQuerySeqs[i].Data())
	  revQuerySeqs[i] = fasta.NewSequence(h, d)
  }
#+end_src
#+begin_export latex
We import byte.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"bytes"
#+end_src
#+begin_export latex
Then we iterate over the range of existing sequences and allocate
space for the array of matches lengths, one for the array of subject
IDs and a map between the subject ID and subject file names.
#+end_export
#+begin_src go <<Allocate space for match lengths and subject ID, Ch. \ref{ch:prep}>>=
  for _, querySeq := range querySeqs {
	  n := len(querySeq.Data())
	  matchLengths := make([]int,n)
	  subjectID := make([]int,n)
	  sID := make(map[int]string)

	  for j := 0 ; j < n; j++ {
		  matchLengths[j] = -1
		  subjectID[j] = -1
	  }
	  //<<Stream against subject files, Ch. \ref{ch:prep}>>
  }
  f.Close()
#+end_src
#+begin_export latex
We then iterate, map the subject files to an ID and read the
sequences. After streaming the files we write the matches lengths to a
file.
#+end_export
#+begin_src go <<Stream against subject files, Ch. \ref{ch:prep}>>=
  for j, subject := range subjectNames {
	  p := *optS + "/" + subject
	  f, err := os.Open(p)
	  util.Check(err)
	  //<<Map subject files to ID>>
	  //<<Read subject files, Ch. \ref{ch:prep}>>
  }
  	  //<<Write matches to file, Ch. \ref{ch:prep}>>

#+end_src
#+begin_export latex
We create a map of the subject files.
#+end_export
#+begin_src go <<Map subject files to ID>>=
  sID[j] = subject
#+end_src
#+begin_export latex
We first read the subject files, then we aggregate smaller sequences
or contigs until they reach the maximum subject length, \ty{msl},
which was calculated above. Note that the aggregation is done only
within files and not across.

After aggregating the sequences we calculate the enhanced suffix array
of the subject and calculate the matches lengths.
#+end_export
#+begin_src go <<Read subject files, Ch. \ref{ch:prep}>>=
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  s := sc.Sequence()
	  d := s.Data()
	  h := []byte(s.Header())

	  for len(d) < msl && sc.ScanSequence() {
		  s = sc.Sequence()
		  h = append(h, '|')
		  h = append(h, []byte(s.Header())...)
		  d = append(d, s.Data()...)
	  }
	  //<<Calculate enhanced suffix array, Ch. \ref{ch:prep}>>
  	  //<<Update match lengths and IDs, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We calculate the enhanced suffix array or \ty{esa} of the subject
sequences.
#+end_export
#+begin_src go <<Calculate enhanced suffix array, Ch. \ref{ch:prep}>>=
  d = bytes.ToUpper(d)
  e := esa.MakeEsa(d)
#+end_src
#+begin_export latex
We import esa.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/esa"
#+end_src
#+begin_export latex
We update the array of matches lengths and subjects IDs.
#+end_export
#+begin_src go <<Update match lengths and IDs, Ch. \ref{ch:prep}>>=
  q := querySeq.Data()

  mat.UpdateMatchLengths(q, e, j, matchLengths, subjectID)
#+end_src
#+begin_export latex
We import the mat package.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
  "github.com/evolbioinf/alfy/mat"
#+end_src
#+begin_export latex
After finishing streaming the subject files and updating the arrays of
matches lengths and IDs we write them out to file.
#+end_export
#+begin_src go <<Write matches to file, Ch. \ref{ch:prep}>>=
  f, err = os.Create(querySeq.Header() + ".txt")
  util.Check(err)

  wr := bufio.NewWriter(f)

  //<<Write output file header, Ch. \ref{ch:prep}>>
  //<<Write matches and subject ID, Ch. \ref{ch:prep}>>
  err = wr.Flush()
  util.Check(err)

  f.Close()
#+end_src
#+begin_export latex
We import the bufio writer.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"bufio"
#+end_src
#+begin_export latex
The header consists of a single line that starts with a hash followed
by the name of the query file and a key/value map to the subject
id. The pairs are written in the following manner :
\ty{subject\_id=subject\_file}.

We start by constructing the list of existing key/value pairs that are
contained inside the subjectID array. Then we print it out in a single
line.
#+end_export
#+begin_src go <<Write output file header, Ch. \ref{ch:prep}>>=
  var pair []string
  seen := make(map[string]bool)

  //<<Check if key/value exists, Ch. \ref{ch:prep}>>
  //<<Write header, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
For every ID present in the subjectID array, we map it to the map of
subject IDs previously contructed and if its the first time it appears
we append it to the list of key-values to print.
#+end_export
#+begin_src go <<Check if key/value exists, Ch. \ref{ch:prep}>>=
  for _, ID := range subjectID {
	  if seq, exists := sID[ID]; exists {
		  if !seen[seq] {
			  pair = append(pair,
				  fmt.Sprintf("%d=%s",
					  ID,seq))
			  seen[seq] = true
		  }
	  }
  }
#+end_src
#+begin_export latex
Then we write the header to the file. First we print the hash
character denoting the header and the name of the query file.  Then we
iterate over the pairs of subject IDs and filenames and print them
into the same line.

At the end we print a newline.
#+end_export
#+begin_src go <<Write header, Ch. \ref{ch:prep}>>=
  fmt.Fprintf(wr, "#%s\t", query)
  fmt.Fprintf(wr, "%s", strings.Join(pair, "\t"))
  fmt.Fprintf(wr,"\n")
#+end_src
#+begin_export latex
We import strings.
#+end_export
#+begin_src go <<Imports, Ch. \ref{ch:prep}>>=
"strings"
#+end_src
#+begin_export latex
Then we write the matches and corresponding subject IDs where they
were found. For this we iterate over the length of the matchesLength,
and for each position print the corresponding matche length and the
subject ID.
#+end_export
#+begin_src go <<Write matches and subject ID, Ch. \ref{ch:prep}>>=
  for j := 0; j < len(matchLengths); j++ {
	  fmt.Fprintf(wr, "%d\t%d\n",
		  matchLengths[j],
		  subjectID[j])
  }
#+end_src
#+begin_export latex
We have now finished \ty{prepAlfy}, which still needs to be tested.

\section{Testing}
The program to test \ty{prepAlfy} has hooks for imports and the
testing logic.
#+end_export
#+begin_src go <<prepAlfy_test.go>>=
  package main

  import(
	  //<<Testing imports, Ch. \ref{ch:prep}>>
  )

  func TestPrepAlfy(t *testing.T) {
	  //<<Testing, Ch. \ref{ch:prep}>>
  }
#+end_src
#+begin_export latex
We import testing.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
  "testing"
#+end_src
#+begin_export latex
We construct a set of tests.
#+end_export
#+begin_src go <<Testing, Ch. \ref{ch:prep}>>=
  var tests []*exec.Cmd

  //<<Construct tests, Ch. \ref{ch:prep}>>
  //<<Run test, Ch. \ref{ch:prep}>>
#+end_src
#+begin_export latex
We import exec.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
"os/exec"
#+end_src
#+begin_export latex
We test the program by setting the queries and the subjects.
#+end_export
#+begin_src go <<Construct tests, Ch. \ref{ch:prep}>>=
  p := "./prepAlfy"
  q := "query"
  s := "subject"

  test := exec.Command(p, "-q", q, "-s", s)
  tests = append(tests, test)
#+end_src
#+begin_export latex
We run the test and compare the result from the first query sequence
with the one we want stored in the file r1.txt.
#+end_export
#+begin_src go <<Run test, Ch. \ref{ch:prep}>>=
  _, err := test.Output()
  if err != nil {
	  t.Error(err)
  }

  get, err := os.ReadFile("q1.txt")
  if err != nil{
	  t.Error(err)
	  }

  f := "r" + strconv.Itoa(1)+".txt"
  want, err := os.ReadFile(f)
  if err != nil {
	  t.Error(err)
  }

  if !bytes.Equal(get,want) {
	  t.Errorf("get:\n%s\nwant:\n%s\n", get, want)
  }
#+end_src
#+begin_export latex
We import os, strconv and bytes.
#+end_export
#+begin_src go <<Testing imports, Ch. \ref{ch:prep}>>=
  "os"
  "strconv"
  "bytes"
#+end_src
